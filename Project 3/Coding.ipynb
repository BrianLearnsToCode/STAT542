{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee43e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4fdd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "115eb38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = [\n",
    "#     'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'their', 'they',\n",
    "#     'his', 'her', 'she', 'he', 'a', 'an', 'and', 'is', 'was', 'are', 'were', 'him', 'himself', 'has', 'have',\n",
    "#     'it', 'its', 'the', 'us'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7fd2e1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262.00023889541626"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_url = 'proj3_data/split_1/train.tsv'\n",
    "test_url = 'proj3_data/split_1/test.tsv'\n",
    "test_y_url = 'proj3_data/split_1/test_y.tsv'\n",
    "train = pd.read_csv(train_url, sep='\\t', header=0, dtype=str)\n",
    "test =  pd.read_csv(test_url, sep='\\t', header=0, dtype=str)\n",
    "train['review'] = train['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "test['review'] = test['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "test_y = pd.read_csv(test_y_url, sep='\\t', header=0,dtype=str)\n",
    "test = test.merge(test_y, on = 'id')\n",
    "\n",
    "tot = pd.DataFrame()\n",
    "tot['review'] = pd.concat([train['review'],test['review']])\n",
    "tot['sentiment'] = pd.concat([train['sentiment'],test['sentiment']])\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=lambda x: x.lower(),  # Convert to lowercase\n",
    "    stop_words=stop_words,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                        # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\" # Use word tokenizer: See Ethan's comment below\n",
    ")\n",
    "\n",
    "dtm_tot = vectorizer.fit_transform(tot['review'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dtm_train=dtm_tot[:25000,:]\n",
    "logreg = LogisticRegression(C=1.01,\n",
    "                            max_iter=10000, \n",
    "                            solver='liblinear',penalty='l1').fit(dtm_train, train['sentiment'])\n",
    "selected_features = feature_names[logreg.coef_[0]!=0]\n",
    "vocabulary_dict = {feature: idx for idx, feature in enumerate(selected_features)}\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838a052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83ef5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=lambda x: x.lower(),  # Convert to lowercase\n",
    "    stop_words=stop_words,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                        # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\" # Use word tokenizer: See Ethan's comment below\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0522832",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_tot = vectorizer.fit_transform(tot['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b7f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700ba0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17549,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab6b54d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x17549 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5256129 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03aad23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_train=dtm_tot[:25000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42916f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1.01,max_iter=10000, solver='liblinear',penalty='l1').fit(dtm_train, train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27a689ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8384f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = feature_names[logreg.coef_[0]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9d8998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_dict = {feature: idx for idx, feature in enumerate(selected_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1260e4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad10b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71ab5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scorer = make_scorer(roc_auc_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "979bc9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_1 is processed\n",
      "time eclapsed: 36.56 seconds\n",
      "split_1 auc score 0.9601\n",
      "------------------\n",
      "split_2 is processed\n",
      "time eclapsed: 32.73 seconds\n",
      "split_2 auc score 0.9623\n",
      "------------------\n",
      "split_3 is processed\n",
      "time eclapsed: 35.31 seconds\n",
      "split_3 auc score 0.9623\n",
      "------------------\n",
      "split_4 is processed\n",
      "time eclapsed: 34.97 seconds\n",
      "split_4 auc score 0.9628\n",
      "------------------\n",
      "split_5 is processed\n",
      "time eclapsed: 38.76 seconds\n",
      "split_5 auc score 0.9628\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "aucs = []\n",
    "processing_times = []\n",
    "start = time.time()\n",
    "for i in range(1,6):    \n",
    "    train_url = f'proj3_data/split_{i}/train.tsv'\n",
    "    test_url = f'proj3_data/split_{i}/test.tsv'\n",
    "    test_y_url = f'proj3_data/split_{i}/test_y.tsv'\n",
    "    \n",
    "    train = pd.read_csv(train_url, sep='\\t', header=0, dtype=str)\n",
    "    test =  pd.read_csv(test_url, sep='\\t', header=0, dtype=str)\n",
    "    train['review'] = train['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "    test['review'] = test['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "    test_y = pd.read_csv(test_y_url, sep='\\t', header=0,dtype=str)\n",
    "    test = test.merge(test_y, on = 'id')\n",
    "    \n",
    "    file_path = 'myvocab.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    vocabulary_dict_loaded = {}\n",
    "    for line in lines:\n",
    "        # Split each line into key and value\n",
    "        key, value = line.strip().split(': ')\n",
    "        # Convert the value to an integer (assuming the values are integers)\n",
    "        vocabulary_dict_loaded[key] = int(value)\n",
    "    \n",
    "    new_vectorizer = TfidfVectorizer(\n",
    "        vocabulary = vocabulary_dict_loaded,\n",
    "        preprocessor=lambda x: x.lower(),  # Convert to lowercase\n",
    "        stop_words=stop_words,             # Remove stop words\n",
    "        ngram_range=(1, 2),               # Use 1- to 4-grams\n",
    "        min_df=0.001,                        # Minimum term frequency\n",
    "        max_df=0.5,                       # Maximum document frequency\n",
    "        token_pattern=r\"\\b[\\w+\\|']+\\b\" # Use word tokenizer: See Ethan's comment below\n",
    "    )\n",
    "\n",
    "    \n",
    "    dtm_train = new_vectorizer.fit_transform(train['review'])   \n",
    "    dtm_test = new_vectorizer.transform(test['review'])\n",
    "    \n",
    "#    param_grid = {\n",
    "#    'learning_rate': [0.2, 0.3,0.4],\n",
    "#    'n_estimators': [500,700,1000]\n",
    "#    'learning_rate': [0.01, 0.1, 0.2],\n",
    "#    'min_child_weight': [1, 3, 5],\n",
    "#    'subsample': [0.8, 0.9, 1.0],\n",
    "#    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "#    'gamma': [0, 0.1, 0.2]\n",
    "#    }\n",
    "#    grid_search = GridSearchCV(XGBClassifier(\n",
    "#                                            max_depth = 5,\n",
    "#                                             n_estimators = 1000,\n",
    "#                                             learning_rate = 0.4,\n",
    "#                                             use_label_encoder=False, \n",
    "#                                             min_child_weight = 5,\n",
    "#                                             subsample = 0.8,\n",
    "#                                             colsample_bytree = 1.0,                                            \n",
    "#                                              eval_metric='logloss', \n",
    "#                                              objective='binary:logistic'),\n",
    "#                            param_grid=param_grid,\n",
    "#                            cv=5,\n",
    "#                            scoring=auc_scorer,\n",
    "#                            n_jobs=-1)\n",
    "#     grid_search.fit(dtm_train, train['sentiment'].astype(int))\n",
    "#     best_params = grid_search.best_params_\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     proba_positive_class = best_model.predict_proba(dtm_test)[:,1]\n",
    "    \n",
    "    xgb_clf = XGBClassifier(\n",
    "                            max_depth = 4,\n",
    "                            n_estimators = 500,\n",
    "                            learning_rate = 0.2,\n",
    "                            use_label_encoder=False, \n",
    "                            min_child_weight = 6,\n",
    "                            eval_metric='logloss', \n",
    "                            objective='binary:logistic').fit(dtm_train, train['sentiment'].astype(int))\n",
    "    \n",
    "    logreg_cv = LogisticRegressionCV(solver='liblinear').fit(dtm_train, train['sentiment'])\n",
    "    \n",
    "    proba_positive_class_1 = logreg_cv.predict_proba(dtm_test)[:,1]\n",
    "    proba_positive_class_2 = xgb_clf.predict_proba(dtm_test)[:,1]\n",
    "    auc = roc_auc_score(test['sentiment'], 0.7*proba_positive_class_1+0.3*proba_positive_class_2)\n",
    "#    auc = roc_auc_score(test['sentiment'], proba_positive_class)\n",
    "    aucs.append(round(auc,3))\n",
    "    \n",
    "    end = time.time()\n",
    "    processing_times.append(round(end-start,2))\n",
    "    \n",
    "    print('split_%i is processed'%i)\n",
    "    print('time eclapsed: %0.2f seconds'%(end - start))\n",
    "    print('split_%i auc score %0.4f'%(i,auc))\n",
    "    print('------------------')\n",
    "    start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "37ba9c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.96, 0.962, 0.962, 0.963, 0.963]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7b605f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36.56, 32.73, 35.31, 34.97, 38.76]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8065d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'myvocab.txt'\n",
    "\n",
    "# Write the dictionary to the text file\n",
    "with open(file_path, 'w') as file:\n",
    "    for word, index in vocabulary_dict.items():\n",
    "        file.write(f'{word}: {index}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f46dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'myvocab.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "vocabulary_dict_loaded = {}\n",
    "for line in lines:\n",
    "    # Split each line into key and value\n",
    "    key, value = line.strip().split(': ')\n",
    "    # Convert the value to an integer (assuming the values are integers)\n",
    "    vocabulary_dict_loaded[key] = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a27ce039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary_dict_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "31a42853",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypred = pd.DataFrame()\n",
    "mypred['id'] = test['id']\n",
    "mypred['prob'] = 0.7*proba_positive_class_1+0.3*proba_positive_class_2\n",
    "\n",
    "file_path = 'mypred.csv'\n",
    "test_pred.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec38b072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40625</td>\n",
       "      <td>0.950597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14191</td>\n",
       "      <td>0.581868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5011</td>\n",
       "      <td>0.049003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23277</td>\n",
       "      <td>0.891420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29766</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>35113</td>\n",
       "      <td>0.107084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>20566</td>\n",
       "      <td>0.044944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>21602</td>\n",
       "      <td>0.000550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>11034</td>\n",
       "      <td>0.640018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>1025</td>\n",
       "      <td>0.923536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      prob\n",
       "0      40625  0.950597\n",
       "1      14191  0.581868\n",
       "2       5011  0.049003\n",
       "3      23277  0.891420\n",
       "4      29766  0.000079\n",
       "...      ...       ...\n",
       "24995  35113  0.107084\n",
       "24996  20566  0.044944\n",
       "24997  21602  0.000550\n",
       "24998  11034  0.640018\n",
       "24999   1025  0.923536\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0f633ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40625</td>\n",
       "      <td>The first and second seasons started off shaki...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14191</td>\n",
       "      <td>As Americans, we have come to expect crapiness...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5011</td>\n",
       "      <td>PERHAPS SPOILER !! well, i ve seen it at the f...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23277</td>\n",
       "      <td>This was one of my favorites as a child. My fa...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29766</td>\n",
       "      <td>I feel it is my duty as a lover of horror film...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>35113</td>\n",
       "      <td>1st watched 8/3/2003 - 2 out of 10(Dir-Brad Sy...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>20566</td>\n",
       "      <td>\\Glen or Glenda\\\" was Edward D. Wood Jr's firs...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>21602</td>\n",
       "      <td>I've seen a lot of bad movies in my life. Date...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>11034</td>\n",
       "      <td>Matt Cvetic is a loyal communist in a Pittsbur...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>1025</td>\n",
       "      <td>In this episode, Locke and Eko go searching fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review sentiment  \\\n",
       "0      40625  The first and second seasons started off shaki...         1   \n",
       "1      14191  As Americans, we have come to expect crapiness...         0   \n",
       "2       5011  PERHAPS SPOILER !! well, i ve seen it at the f...         0   \n",
       "3      23277  This was one of my favorites as a child. My fa...         1   \n",
       "4      29766  I feel it is my duty as a lover of horror film...         0   \n",
       "...      ...                                                ...       ...   \n",
       "24995  35113  1st watched 8/3/2003 - 2 out of 10(Dir-Brad Sy...         0   \n",
       "24996  20566  \\Glen or Glenda\\\" was Edward D. Wood Jr's firs...         0   \n",
       "24997  21602  I've seen a lot of bad movies in my life. Date...         0   \n",
       "24998  11034  Matt Cvetic is a loyal communist in a Pittsbur...         0   \n",
       "24999   1025  In this episode, Locke and Eko go searching fo...         1   \n",
       "\n",
       "      score  \n",
       "0        10  \n",
       "1         3  \n",
       "2         2  \n",
       "3         8  \n",
       "4         1  \n",
       "...     ...  \n",
       "24995     2  \n",
       "24996     1  \n",
       "24997     1  \n",
       "24998     4  \n",
       "24999     9  \n",
       "\n",
       "[25000 rows x 4 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef770a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
