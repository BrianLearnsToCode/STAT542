{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "795476e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4eeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3d50f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tmp = pd.to_datetime(data['Date'])\n",
    "    data['Wk'] = tmp.dt.isocalendar().week\n",
    "    data['Yr'] = tmp.dt.year\n",
    "    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])  # 52 weeks \n",
    "    data['IsHoliday'] = data['IsHoliday'].apply(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a7cc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = 'train.csv'\n",
    "test_url = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d19011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Reading train data\n",
    "    file_path = train_url\n",
    "    train = pd.read_csv(file_path)\n",
    "\n",
    "    # Reading test data\n",
    "    file_path = test_url\n",
    "    test = pd.read_csv(file_path)\n",
    "    \n",
    "    #SVD \n",
    "    d = 8\n",
    "    test_depts = test['Dept'].unique()\n",
    "    train_new = pd.DataFrame()\n",
    "    for dept in test_depts:\n",
    "        train_dept_data = train[train['Dept'] == dept]        \n",
    "        selected_columns = train_dept_data[['Store', 'Date', 'Weekly_Sales']]\n",
    "\n",
    "        pivoted=selected_columns.pivot(index='Date', columns='Store', values='Weekly_Sales').fillna(0)\n",
    "        index_names = pivoted.index\n",
    "        column_names = pivoted.columns\n",
    "        train_dept_ts = np.array(pivoted)\n",
    "        \n",
    "        store_mean = np.mean(train_dept_ts,axis=0)\n",
    "        train_dept_ts_new = train_dept_ts - store_mean\n",
    "        U, S, V = np.linalg.svd(train_dept_ts_new)\n",
    "        d_max = min(d,len(S))\n",
    "        X_new = np.dot(U[:,:d_max],np.dot(np.diag(S[:d_max]),V[:d_max,:]))+ store_mean\n",
    "        df = pd.DataFrame(X_new, columns=column_names, index=index_names)\n",
    "        train_dept_new = pd.melt(df.reset_index(),id_vars='Date', var_name='Store',value_name='Weekly_Sales_SVD')\n",
    "        train_dept_new['Dept'] = dept\n",
    "        train_new = pd.concat([train_new, train_dept_new], ignore_index=True)\n",
    "    train = train.merge(train_new,on = ['Date','Store','Dept'],how='left').fillna(0).drop('Weekly_Sales',axis=1)\n",
    "    \n",
    "\n",
    "    # pre-allocate a pd to store the predictions\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    train_pairs = train[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])\n",
    "\n",
    "    train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')\n",
    "    train_split = preprocess(train_split)\n",
    "    y, X = patsy.dmatrices('Weekly_Sales_SVD ~ Weekly_Sales_SVD + Store + Dept + Yr  + Wk+ IsHoliday', \n",
    "                           data = train_split, \n",
    "                           return_type='dataframe')\n",
    "    train_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "\n",
    "    test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "    test_split = preprocess(test_split)\n",
    "    y, X = patsy.dmatrices('Yr ~ Store + Dept + Yr  + Wk+ IsHoliday', \n",
    "                           data = test_split, \n",
    "                           return_type='dataframe')\n",
    "    X['Date'] = test_split['Date']\n",
    "    test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "    keys = list(train_split)\n",
    "\n",
    "    for key in keys:\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "        holidays = X_test['IsHoliday']\n",
    "\n",
    "        Y = X_train['Weekly_Sales_SVD']\n",
    "        X_train = X_train.drop(['Weekly_Sales_SVD','Store', 'Dept', 'IsHoliday'], axis=1)\n",
    "        X_test = X_test.drop(['IsHoliday'], axis=1)\n",
    "        \n",
    "        X_train['Yr_square'] = X_train['Yr']**2\n",
    "        X_test['Yr_square'] = X_test['Yr']**2\n",
    "        \n",
    "\n",
    "        cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        cols_to_drop = []\n",
    "        for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "            col_name = X_train.columns[i]\n",
    "            # Extract the current column and all previous columns\n",
    "            tmp_Y = X_train.iloc[:, i].values\n",
    "            tmp_X = X_train.iloc[:, :i].values\n",
    "\n",
    "            coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "            if np.sum(residuals) < 1e-16:\n",
    "                    cols_to_drop.append(col_name)\n",
    "\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "        \n",
    "\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "        \n",
    "        tmp_pred = X_test[['Store', 'Dept', 'Date']]\n",
    "        X_test = X_test.drop(['Store', 'Dept', 'Date'], axis=1)\n",
    "\n",
    "        tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)\n",
    "        tmp_pred['IsHoliday'] = holidays.apply(bool)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "\n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "    file_path = 'mypred.csv'\n",
    "    test_pred.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199cf93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b29015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold1 is processed\n",
      "time eclapsed: 25.48 seconds\n",
      "fold2 is processed\n",
      "time eclapsed: 27.48 seconds\n",
      "fold3 is processed\n",
      "time eclapsed: 33.62 seconds\n",
      "fold4 is processed\n",
      "time eclapsed: 34.62 seconds\n",
      "fold5 is processed\n",
      "time eclapsed: 39.62 seconds\n",
      "fold6 is processed\n",
      "time eclapsed: 38.81 seconds\n",
      "fold7 is processed\n",
      "time eclapsed: 43.69 seconds\n",
      "fold8 is processed\n",
      "time eclapsed: 48.75 seconds\n",
      "fold9 is processed\n",
      "time eclapsed: 43.49 seconds\n",
      "fold10 is processed\n",
      "time eclapsed: 43.70 seconds\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "processing_times = []\n",
    "start = time.time()\n",
    "for j in range(1, num_folds + 1):\n",
    "    \n",
    "    \n",
    "    # Reading train data\n",
    "    file_path = f'Proj2_Data/fold_{j}/train.csv'\n",
    "    train = pd.read_csv(file_path)\n",
    "\n",
    "    # Reading test data\n",
    "    file_path = f'Proj2_Data/fold_{j}/test.csv'\n",
    "    test = pd.read_csv(file_path)\n",
    "    \n",
    "    #SVD \n",
    "    d = 8\n",
    "    test_depts = test['Dept'].unique()\n",
    "    train_new = pd.DataFrame()\n",
    "    for dept in test_depts:\n",
    "        train_dept_data = train[train['Dept'] == dept]        \n",
    "        selected_columns = train_dept_data[['Store', 'Date', 'Weekly_Sales']]\n",
    "\n",
    "        pivoted=selected_columns.pivot(index='Date', columns='Store', values='Weekly_Sales').fillna(0)\n",
    "        index_names = pivoted.index\n",
    "        column_names = pivoted.columns\n",
    "        train_dept_ts = np.array(pivoted)\n",
    "        \n",
    "        store_mean = np.mean(train_dept_ts,axis=0)\n",
    "        train_dept_ts_new = train_dept_ts - store_mean\n",
    "        U, S, V = np.linalg.svd(train_dept_ts_new)\n",
    "        d_max = min(d,len(S))\n",
    "        X_new = np.dot(U[:,:d_max],np.dot(np.diag(S[:d_max]),V[:d_max,:]))+ store_mean\n",
    "#        X_new = np.dot(U[:,:len(S)],np.dot(np.diag(S),V[:len(S),:]))+ store_mean\n",
    "#        if np.sum(X_new-train_dept_ts_new) > 1e-8:\n",
    "#            print\n",
    "        df = pd.DataFrame(X_new, columns=column_names, index=index_names)\n",
    "        train_dept_new = pd.melt(df.reset_index(),id_vars='Date', var_name='Store',value_name='Weekly_Sales_SVD')\n",
    "        train_dept_new['Dept'] = dept\n",
    "        train_new = pd.concat([train_new, train_dept_new], ignore_index=True)\n",
    "    train = train.merge(train_new,on = ['Date','Store','Dept'],how='left').fillna(0).drop('Weekly_Sales',axis=1)\n",
    "    \n",
    "\n",
    "    # pre-allocate a pd to store the predictions\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    train_pairs = train[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])\n",
    "\n",
    "    train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')\n",
    "    train_split = preprocess(train_split)\n",
    "    y, X = patsy.dmatrices('Weekly_Sales_SVD ~ Weekly_Sales_SVD + Store + Dept + Yr  + Wk+ IsHoliday', \n",
    "                           data = train_split, \n",
    "                           return_type='dataframe')\n",
    "    train_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "\n",
    "    test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "    test_split = preprocess(test_split)\n",
    "    y, X = patsy.dmatrices('Yr ~ Store + Dept + Yr  + Wk+ IsHoliday', \n",
    "                           data = test_split, \n",
    "                           return_type='dataframe')\n",
    "    X['Date'] = test_split['Date']\n",
    "    test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "    keys = list(train_split)\n",
    "\n",
    "    for key in keys:\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "        holidays = X_test['IsHoliday']\n",
    "\n",
    "        Y = X_train['Weekly_Sales_SVD']\n",
    "#        weights = 1/np.sqrt(X_train['IsHoliday']*4 + 1)\n",
    "        X_train = X_train.drop(['Weekly_Sales_SVD','Store', 'Dept', 'IsHoliday'], axis=1)\n",
    "        X_test = X_test.drop(['IsHoliday'], axis=1)\n",
    "        \n",
    "        X_train['Yr_square'] = X_train['Yr']**2\n",
    "        X_test['Yr_square'] = X_test['Yr']**2\n",
    "        \n",
    "\n",
    "        cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        cols_to_drop = []\n",
    "        for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "            col_name = X_train.columns[i]\n",
    "            # Extract the current column and all previous columns\n",
    "            tmp_Y = X_train.iloc[:, i].values\n",
    "            tmp_X = X_train.iloc[:, :i].values\n",
    "\n",
    "            coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "            if np.sum(residuals) < 1e-16:\n",
    "                    cols_to_drop.append(col_name)\n",
    "\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "        \n",
    "\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "#        model = sm.WLS(Y, X_train,weights).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "\n",
    "        tmp_pred = X_test[['Store', 'Dept', 'Date']]\n",
    "        X_test = X_test.drop(['Store', 'Dept', 'Date'], axis=1)\n",
    "\n",
    "        tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)\n",
    "        tmp_pred['IsHoliday'] = holidays\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "\n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "    file_path = f'Proj2_Data/fold_{j}/mypred.csv'\n",
    "    test_pred.to_csv(file_path, index=False)\n",
    "    \n",
    "    end = time.time()\n",
    "    processing_times.append(round(end-start,2))\n",
    "    \n",
    "    print('fold%i is processed'%j)\n",
    "    print('time eclapsed: %0.2f seconds'%(end - start))\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e69775c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myeval():\n",
    "    file_path = 'Proj2_Data/test_with_label.csv'\n",
    "    test_with_label = pd.read_csv(file_path)\n",
    "    num_folds = 10\n",
    "    wae = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        file_path = f'Proj2_Data/fold_{i+1}/test.csv'\n",
    "        test = pd.read_csv(file_path)\n",
    "        test = test.drop(columns=['IsHoliday']).merge(test_with_label, on=['Date', 'Store', 'Dept'])\n",
    "\n",
    "        file_path = f'Proj2_Data/fold_{i+1}/mypred.csv'\n",
    "        test_pred = pd.read_csv(file_path)\n",
    "\n",
    "        # Left join with the test data\n",
    "        new_test = test_pred.merge(test, on=['Date', 'Store', 'Dept'], how='left')\n",
    "\n",
    "        # Compute the Weighted Absolute Error\n",
    "        actuals = new_test['Weekly_Sales']\n",
    "        preds = new_test['Weekly_Pred']\n",
    "        weights = new_test['IsHoliday_x'].apply(lambda x: 5 if x else 1)\n",
    "        wae.append(sum(weights * abs(actuals - preds)) / sum(weights))\n",
    "\n",
    "    return wae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3edb10e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1947.662\n",
      "\t1391.496\n",
      "\t1393.792\n",
      "\t1524.559\n",
      "\t2318.163\n",
      "\t1637.733\n",
      "\t1615.955\n",
      "\t1362.799\n",
      "\t1351.377\n",
      "\t1332.592\n",
      "1587.613\n"
     ]
    }
   ],
   "source": [
    "wae = myeval()\n",
    "for value in wae:\n",
    "    print(f\"\\t{value:.3f}\")\n",
    "print(f\"{sum(wae) / len(wae):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "808f6e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1947.7,\n",
       " 1391.5,\n",
       " 1393.8,\n",
       " 1524.6,\n",
       " 2318.2,\n",
       " 1637.7,\n",
       " 1616.0,\n",
       " 1362.8,\n",
       " 1351.4,\n",
       " 1332.6]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[round(ele,1) for ele in wae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cc4e1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25.48, 27.48, 33.62, 34.62, 39.62, 38.81, 43.69, 48.75, 43.49, 43.7]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dfe597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ad231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bfa421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ed103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddedb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
